# System/default
import sys
import os

# Arguments
import argparse
import codecs


from resemblyzer import preprocess_wav, VoiceEncoder
from demo_utils import *
from pathlib import Path
from tqdm import tqdm
import numpy as np


# DEMO 05: In this demo we'll show how we can achieve a modest form of fake speech detection with 
# Resemblyzer. This method assumes you have some reference audio for the target speaker that you 
# know is real, so it is not a universal fake speech detector on its own.
# In the audio data directory we have 18 segments of Donald Trump. 12 are real and extracted from
# actual speeches, while the remaining 6 others are fake and generated by various users on 
# youtube, with a high discrepancy of voice cloning quality and naturalness achieved. We will 
# take 6 segments of real speech as ground truth reference and compare those against the 12 
# remaining. Those segments are selected at random, so will run into different results every time
# you run the script, but they should be more or less consistent.
# Using the voice of Donald Trump is merely a matter of convenience, as several fake speeches 
# with his voice were already put up on youtube. This choice was not politically motivated.

# Two list as arguments real/ fake list

parser = argparse.ArgumentParser(description="")

# Add options
parser.add_argument("-l", "--log_file", default=None, help="Logger file")
parser.add_argument("-v", "--verbosity", action="count", default=0, help="increase output verbosity")
parser.add_argument("--realConfig",dest='real_config_name',type=str, help="increase output verbosity")
parser.add_argument("--fakeConfig",dest='fake_config_name',type=str, help="increase output verbosity")
# Add arguments
parser.add_argument("real_files")
parser.add_argument("conv_files")

args = parser.parse_args()


## Load and preprocess the audio
real_files=args.real_files
conv_files=args.conv_files
real_config_name=args.real_config_name
fake_config_name=args.fake_config_name	

import pandas as pd

#real
real_df=pd.read_csv(real_files)
real_df['speaker']=[os.path.basename(rf).split('_')[0].strip()  for rf in real_df['sound'].to_list()]
real_df['label']=['real' for _ in real_df['sound'].to_list()]

real_df['names']=[os.path.basename(fname).split('.')[0] for fname in real_df['sound'].to_list()]



# fake
conv_df=pd.read_csv(conv_files)
conv_df['speaker']=[os.path.basename(cf).split('_')[0].strip()  for cf in conv_df['sound'].to_list()]
conv_df['label']=['fake' for _ in conv_df['sound'].to_list()]

conv_df['names']=[os.path.basename(fname).split('.')[0] for fname in conv_df['sound'].to_list()]


grouped_by_speaker= real_df.groupby("speaker")

grt_label=[]
prd_label=[]

for  speaker_grp in  grouped_by_speaker:
	real_speaker,real_data=speaker_grp
	conv_data=conv_df.iloc[real_data.index]








	wav_fpaths=real_data["sound"].to_list()+conv_data['sound'].to_list()
	
	wavs = [preprocess_wav(wav_fpath) for wav_fpath in \
					tqdm(wav_fpaths, "Preprocessing wavs", len(wav_fpaths), unit=" utterances")]


	# # # # # ## Compute the embeddings
	encoder = VoiceEncoder()
	embeds = np.array([encoder.embed_utterance(wav) for wav in wavs])


	speakers =np.array(real_data["label"].to_list()+conv_data['label'].to_list())
	names = np.array(real_data["names"].to_list()+conv_data['names'].to_list())

	names_ref=names
	speakers_ref=speakers


	training_set_len=len(np.where(speakers == "real"))/2

	# Take 6 real embeddings at random, and leave the 6 others for testing
	gt_indices = np.random.choice(*np.where(speakers == "real"), training_set_len, replace=False) 
	mask = np.zeros(len(embeds), dtype=np.bool)
	mask[gt_indices] = True
	gt_embeds = embeds[mask]
	gt_names = names[mask]
	gt_speakers = speakers[mask]
	embeds, speakers, names = embeds[~mask], speakers[~mask], names[~mask]


	## Compare all embeddings against the ground truth embeddings, and compute the average similarities.
	scores = (gt_embeds @ embeds.T).mean(axis=0)

	# Order the scores by decreasing order
	sort = np.argsort(scores)[::-1]
	scores, names, speakers = scores[sort], names[sort], speakers[sort]

	# print(len(speakers))
	# print(scores[speakers=="real"],scores[speakers == "fake"],speakers)

	confusion={"false":0,"true":0}
	prd_label+=speakers.tolist()
	grt_label+=speakers_ref.tolist()[:len(speakers.tolist())]
	



# classification task

from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
cm=confusion_matrix(grt_label, prd_label, labels=['real','fake'])


def compute_metrics(confusion_matrix):
		"""Calculates specificity, sensitivity, accuracy and uar from confusion matrix
		Confusion matrix of form [[tp, fp]
															[fn, tn]]
		args:
			confusion_matrix: 2 by 2 nd-array
			output: tuple of float (specificity, sensitivity, accuracy, uar)
		"""
		cm = confusion_matrix
		tp, tn = cm[0, 0], cm[1, 1]
		fn, fp = cm[1, 0], cm[0, 1]
		
		sensitivity = tp / (tp + fn)
		specificity = tn / (fp + tn)
		
		uar = (specificity + sensitivity)/2.0
		
		accuracy = (tp + tn) / (tp + tn + fp + fn)
		
		metrics_dict = dict(sensitivity=sensitivity, specificity=specificity, 
											 accuracy=accuracy, uar=uar)
		return metrics_dict

print()
print( "{} Vs. {}  Accuracy {} ".format(real_config_name,fake_config_name,compute_metrics(cm)['accuracy']))


# cr=classification_report(grt_label, prd_label, target_names=['real','fake'])
# print(cr)

