# -*- coding: utf-8 -*-
"""alpForcedAlign_with_PS.ipynb

Automatically generated by Colaboratory.

Original file is located at
		https://colab.research.google.com/drive/1l3ffFyZCI8Who8kogPwgxIO34k8zewJ8
"""

import os
import sys
import pandas as pd
import numpy as np





import os
import sys
import pandas as pd
import numpy as np

from audio_manipulation import print_stats,plot_waveform, plot_specgram

import torch
import torchaudio
import torchaudio.functional as F
import torchaudio.transforms as T

print(torch.__version__)
print(torchaudio.__version__)



import argparse


def build_arg_parser():
	parser = argparse.ArgumentParser(description="")
	parser.add_argument("filelist", help="audio file list")
	parser.add_argument("outdir_path", help="text file")
	parser.add_argument('--language',dest='lang',default='fra')
	parser.add_argument('--model_name',dest='model_name',default='fra2105cv')

	return parser



def main():



	metadata = torchaudio.info(sample)
	print(metadata)

	waveform, sample_rate = torchaudio.load(sample)

	print_stats(waveform, sample_rate=sample_rate)
	plot_waveform(waveform, sample_rate)
	plot_specgram(waveform, sample_rate)


	from allosaurus.app import read_recognizer


	audio_file=sample
	lang='fra'
	model='latest'
	# load your model by the <model name>, will use 'latest' if left empty
	model = read_recognizer(model)

	# run inference on <audio_file> with <lang>, lang will be 'ipa' if left empty
	model.recognize(audio_file, lang)

	!python -m allosaurus.bin.download_model -m latest

	!python -m allosaurus.bin.list_phone --lang fra

	phoneset_list_fra="a a̟ b d d̪ e f i j k k̟ l l̪ m n n̪ o oː p r s s̪ t t̪ u v w y z z̪ æ̃ ø ø̞ ŋ œ œ̃ ɑ ɑ̃ ɒ ɒ̃ ɔ ɔ̃ ə ɛ ɛː ɛ̃ ɡ ɡ̟ ɥ ɦ ɲ ʀ ʁ ʃ ʒ".split(' ')

	phoneset_list_fra

	french_phoneset=pd.read_csv('/content/drive/MyDrive/Colab Notebooks/data/french_phoneset_mapping.csv')

	for phone_ipa in phoneset_list_fra:
		if phone_ipa in french_phoneset['IPA'].to_list():
			print(phone_ipa)

		

		else:
			print("this phone is missing or has a particular diacritic {}   ".format(phone_ipa))

	rec_example="ə k i l̪ e v e u n u ŋ l o ʁ ɒ v i a n ʁ o v e ʁ l ə n a f e t ɒ d̪ i o f e i d e s̪ ɒ n e o".split()
	for phone_ipa in rec_example:
		if phone_ipa in french_phoneset['IPA'].to_list():
			print(phone_ipa)

		else:
			print("this phone is missing or has a particular diacritic {}   ".format(phone_ipa))

	import unicodedata
	def strip_accents(s):
		 return ''.join(c for c in unicodedata.normalize('NFD', s)
										if unicodedata.category(c) != 'Mn' and ~)

	rec_example="ə k i l̪ e v e u n u ŋ l o ʁ ɒ v i a n ʁ o v e ʁ l ə n a f e t ɒ d̪ i o f e i d e s̪ ɒ n e o".split()
	liaphon_sample=[]
	for phone_ipa in rec_example:
		if phone_ipa in french_phoneset['IPA'].to_list():
			print("no special diacritic is detected in this phone  {} Valid Phone ".format(phone_ipa))

		else:
			print("this phone is missing or has a particular diacritic {}   ".format(phone_ipa))
			phone_ipa=strip_accents(phone_ipa)
			if phone_ipa in french_phoneset['IPA'].to_list():
				print("after cleaning up the  diacritics this phone {}  become  Valid Phone ".format(phone_ipa))
			else:
				print("this phone is not a valid french phone {} Invalid Phone ".format(phone_ipa))
				phone_ipa='_'
		liaphon_sample.append(french_phoneset.loc[french_phoneset.IPA==phone_ipa, 'liaphon'].values[0])

	print(liaphon_sample)

	!python -m pip install --upgrade pip setuptools wheel

	!pip install pocketsphinx

	"""

	```
	# Ce texte est au format code
	```
	# Convert Ipa to Liaphon

	> Bloc en retrait

	"""

	!git clone https://github.com/cmusphinx/pocketsphinx.git

	!cd pocketsphinx

	!ls

	!cd /content/pocketsphinx

	n_fft = 1024
	win_length = 551
	hop_length = 220

	# define transformation
	spectrogram = T.Spectrogram(
			n_fft=n_fft,
			win_length=win_length,
			hop_length=hop_length,
			center=True,
			pad_mode="reflect",
			power=2.0,
	)
	# Perform transformation
	spec = spectrogram(waveform)

	print_stats(spec)
	plot_spectrogram(spec[0], title='torchaudio')

	phone_rec=model.recognize(audio_file, lang,timestamp=True)
	prev_phone_end=0

	phone_seq_lia=[]

	frame_length=1024
	frame_step=220
	frames_overlap = frame_length - frame_step

	fs=22050


	prev_phone_end=0
	for item in phone_rec.split('\n'):
		start_t,duration,phone_ipa=item.split(' ')
		start_t=float(start_t)
		duration=float(duration)
		cur_phone_end=start_t+duration
		
		if prev_phone_end<start_t:
			 sig_seg=waveform[0,prev_phone_end*fs:start_t*fs]
			 phone_seq_lia.append({'label':'sil',
						'start_t':prev_phone_end,
						'end_t':start_t,
						'duration':start_t-prev_phone_end,
						'frames': 
			})
		sig_seg=waveform[0,prev_phone_end*fs:start_t*fs]
		phone_seq_lia.append({'label':phone_ipa,
						'start_t':start_t,
						'end_t':cur_phone_end,
						'duration':duration,
						'frames':
			})
		prev_phone_end=cur_phone_end

	import numpy as np



	def stride_trick(a, stride_length, stride_step):
			 """
			 apply framing using the stride trick from numpy.

			 Args:
					 a (array) : signal array.
					 stride_length (int) : length of the stride.
					 stride_step (int) : stride step.

			 Returns:
					 blocked/framed array.
			 """
			 nrows = ((a.size - stride_length) // stride_step) + 1
			 n = a.strides[0]
			 return np.lib.stride_tricks.as_strided(a,
																							shape=(nrows, stride_length),
																							strides=(stride_step*n, n))


	def framing(sig, fs=16000, win_len=0.025, win_hop=0.01):
			 """
			 transform a signal into a series of overlapping frames (=Frame blocking).

			 Args:
					 sig     (array) : a mono audio signal (Nx1) from which to compute features.
					 fs        (int) : the sampling frequency of the signal we are working with.
														 Default is 16000.
					 win_len (float) : window length in sec.
														 Default is 0.025.
					 win_hop (float) : step between successive windows in sec.
														 Default is 0.01.

			 Returns:
					 array of frames.
					 frame length.

			 Notes:
			 ------
					 Uses the stride trick to accelerate the processing.
			 """
			 # run checks and assertions
			 if win_len < win_hop: print("ParameterError: win_len must be larger than win_hop.")

			 # compute frame length and frame step (convert from seconds to samples)
			 frame_length = win_len * fs
			 frame_step = win_hop * fs
			 signal_length = len(sig)
			 frames_overlap = frame_length - frame_step

			 # compute number of frames and left sample in order to pad if needed to make
			 # sure all frames have equal number of samples  without truncating any samples
			 # from the original signal
			 rest_samples = np.abs(signal_length - frames_overlap) % np.abs(frame_length - frames_overlap)
			 pad_signal = np.append(sig, np.array([0] * int(frame_step - rest_samples) * int(rest_samples != 0.)))

			 # apply stride trick
			 frames = stride_trick(pad_signal, int(frame_length), int(frame_step))
			 return frames, frame_length

	frames,frame_length=framing(waveform,fs=22050)
	print(len(frames),frame_length,waveform.shape)

	import numpy as np


	 def framing_2(sig, fs=16000, win_len=0.025, win_hop=0.01):
			 """
			 transform a signal into a series of overlapping frames.

			 Args:
					 sig            (array) : a mono audio signal (Nx1) from which to compute features.
					 fs               (int) : the sampling frequency of the signal we are working with.
																		Default is 16000.
					 win_len        (float) : window length in sec.
																		Default is 0.025.
					 win_hop        (float) : step between successive windows in sec.
																		Default is 0.01.

			 Returns:
					 array of frames.
					 frame length.
			 """
			 # compute frame length and frame step (convert from seconds to samples)
			 frame_length = win_len * fs
			 frame_step = win_hop * fs
			 signal_length = len(sig)
			 frames_overlap = frame_length - frame_step

			 # Make sure that we have at least 1 frame+
			 num_frames = np.abs(signal_length - frames_overlap) // np.abs(frame_length - frames_overlap)
			 rest_samples = np.abs(signal_length - frames_overlap) % np.abs(frame_length - frames_overlap)

			 # Pad Signal to make sure that all frames have equal number of samples
			 # without truncating any samples from the original signal
			 if rest_samples != 0:
					 pad_signal_length = int(frame_step - rest_samples)
					 z = np.zeros((pad_signal_length))
					 pad_signal = np.append(sig, z)
					 num_frames += 1
			 else:
					 pad_signal = sig

			 # make sure to use integers as indices
			 frame_length = int(frame_length)
			 frame_step = int(frame_step)
			 num_frames = int(num_frames)

			 # compute indices
			 idx1 = np.tile(np.arange(0, frame_length), (num_frames, 1))
			 idx2 = np.tile(np.arange(0, num_frames * frame_step, frame_step),
											(frame_length, 1)).T
			 indices = idx1 + idx2
			 frames = pad_signal[indices.astype(np.int32, copy=False)]
			 return frames

	phone_rec=model.recognize(audio_file, lang,timestamp=True)
	prev_phone_end=0

	phone_seq_lia=[]


	for item in phone_rec.split('\n'):
		start_t,duration,phone_ipa=item.split(' ')
		start_t=float(start_t)
		duration=float(duration)
		cur_phone_end=start_t+duration
		if prev_phone_end<start_t:
			sig_seg=waveform[0,int(prev_phone_end*fs):int(start_t*fs)+1]

			phone_seq_lia.append({'label':'sil',
				 'start_t':prev_phone_end,
				 'end_t':start_t,
				 'duration':start_t-prev_phone_end,
				 'frames': len(framing(sig_seg,fs=fs)[0])
		})
		sig_seg=waveform[0,int(start_t*fs):int(cur_phone_end*fs)+1]

		phone_seq_lia.append({'label':phone_ipa,
						'start_t':start_t,
						'end_t':cur_phone_end,
						'duration':len(sig_seg)/fs,
						'frames': len(framing(sig_seg,fs=fs)[0])
			})
		prev_phone_end=cur_phone_end

	sig_seg=waveform[0,int(cur_phone_end*fs):waveform.shape[1]+1]
	if cur_phone_end< waveform.shape[1]/fs:
		 phone_seq_lia.append({'label':'sil',
				 'start_t':cur_phone_end,
				 'end_t':waveform.shape[1]/fs,
				 'duration':(waveform.shape[1]/fs)-prev_phone_end,
				 'frames': len(framing(sig_seg,fs=fs)[0])
		})
	print([ (phone['start_t'],phone['end_t'],phone['label'],) for phone in phone_seq_lia])

	print(np.sum([ int(phone['frames']) for phone in phone_seq_lia]))

	len(framing(waveform,fs=22050)[0])